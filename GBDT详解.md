目录
* 1 GBDT思想
* 2 GBDT的分类——回归分类与多元分类
* 3 梯度提升和梯度下降
* 4 GBDT常用的损失函数
* 5 正则化
* 6 优缺点
* 7 sklearn参数
* 8 参考资料

### 1 GBDT思想
&#8195;&#8195;Gradient Boosting是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。算法1描述了Gradient Boosting算法的基本流程，在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新
&#8195;&#8195;采用决策树作为弱分类器的Gradient Boosting算法被称为GBDT，有时又被称为MART（Multiple Additive Regression Tree）。


### 2 GBDT的种类——回归分类与多元分类
&#8195;&#8195;GBDT中的基分类器当然是决策树。但是决策树有很多比如C4.5、ID3、CART等等。那么用的是哪种树？在GBDT里，用的是CART（分类与回归树），同时Sklearn里面实现GBDT时用的基分类器也是CART。一般的CART是这样的：用于分类任务时，树的分裂准则采用基尼指数，用于回归任务时，用MSE（均方误差）。 
注意：当然在回归任务中，分裂准则也不再局限于用MSE，也可以用MAE，还可以用Friedman_mse（改进型的mse)。
&#8195;&#8195;GBDT用于回归和分类任务时，使用的不同的loss function，但其本质还是一样的。下面分别介绍 GBDT回归算法和分类算法。
#### 2.1 GBDT回归算法
![](https://github.com/flysaint/Datawhale-/blob/master/GBDT%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95.png)

算法第1步，是一个初始化的过程。因为每次计算的时候，需要用到前一个模型的预测值，这里会有一个$$F_0(x_i)$$存在，取值大小依赖损失函数的选择，比如损失函数用MSE时，$F_0(x_i)$ = $\overline{y}$，$\overline{y}$为初始样本平均值。

算法第3步，就是我们上面说的损失函数的负梯度在当前模型的值。

算法第4步，其目的就是为了求一个最优的基分类器。对于不同的基分类器有不同的寻找，比如，对于决策树，寻找一个最优的树的过程其实依靠的就是启发式的分裂准则。

算法第5步，是一个Line search 的过程，具体可以参考Friedman的文章。在GBDT里，通常将这个过程作为Shrinkage，也就是把$ρ_m$做为学习率。

算法第6步，求得新的基分类器后，利用加法模型，更新出下一个模型$F_m(x)$

#### 2.2 GBDT分类算法 
GBDT分类算法依据分类的数量，分为二元分类和多元分类算法。
##### 2.2.1 二元分类算法
&#8195;&#8195;其实不论对于回归和分类，GBDT过程几乎一模一样的。最大的不同在于因loss function不同而引起的初始化不同、叶子节点取值不同。
![](https://github.com/flysaint/Datawhale-/blob/master/GBDT%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.png)

&#8195;&#8195;算法3就是GBDT用于分类任务时，loss funcion选用logloss的算法流程。可以看到，和回归任务是一样的，并没有什么特殊的处理环节。（其实在sklearn源码里面，虽然回归任务的模型定义是GradientBoostingRegressor()而分类任务是GradientBoostingClassifier()，但是这两者区分开来是为了方便用户使用，最终两者都是共同继承BaseGradientBoosting()，算法3这些流程都是在BaseGradientBoosting()完成的，GradientBoostingRegressor()、GradientBoostingClassifier()只是完成一些学习器参数配置的任务）
##### 2.2.2 多元分类算法
多元分类算法过程和二分类的GBDT类似，但是有一个地方有很大的不同，下文将详细的介绍。

![](https://github.com/flysaint/Datawhale-/blob/master/GBDT%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.png)

从代码上看，大致和分类时候的过程一样。最大的不同点在于多了一层内部的循环For。
这里需要注意的是： 
1.对于多分类任务，GDBT的做法是采用一对多的策略。
也就是说，对每个类别训练M个分类器。假设有K个类别，那么训练完之后总共有M*K颗树。 
2.两层循环的顺序不能改变。也就是说，K个类别都拟合完第一颗树之后才开始拟合第二颗树，不允许先把某一个类别的M颗树学习完，再学习另外一个类别。

### 3 梯度提升和梯度下降
&#8195;&#8195;如 2 小节中所写，GBDT使用梯度提升实现学习优化（通过负梯度拟合）的过程。
![](index_files/898b5c4c-d5ef-45ce-b30f-63a20e378162.png)
除了梯度提升，梯度下降也是我们常用的学习优化方法。
#### 梯度提升和梯度下降的区别
&#8195;&#8195;下表是梯度提升算法和梯度下降算法的对比情况。可以发现，两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。

![](index_files/041041ab-d147-4cf0-8d22-39130b7eca27.jpg)

### 4. GBDT常用的损失函数
#### 4.1 分类算法损失函数
##### a)指数损失函数
表达式为$$L(y, f(x)) = exp(-yf(x))$$</p>
其负梯度计算和叶子节点的最佳残差拟合参见Adaboost类似。为了不让本文篇幅过于臃肿，这里不做展开，详情可查看《统计学习方法》P145 8.3.2前向分布算法与Adboosting。

##### b)对数损失函数
分为二元分类和多元分类两种,详见 小节2.
#### 4.2 回归算法损失函数
##### a) 均方差
这个是最常见的回归损失函数了$$L(y, f(x)) =(y-f(x))^2$$
##### b) 绝对损失
这个损失函数也很常见$$L(y, f(x)) =|y-f(x)|$$
对应负梯度误差为：$$sign(y_i-f(x_i))$$
##### c) Huber损失
它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：$$L(y, f(x))=<br>\begin{cases}<br>\frac{1}{2}(y-f(x))^2&amp; {|y-f(x)| \leq \delta}\\<br>\delta(|y-f(x)| - \frac{\delta}{2})&amp; {|y-f(x)| &gt; \delta}<br>\end{cases}$$
对应的负梯度误差为：
$$r(y_i, f(x_i))=<br>\begin{cases}<br>y_i-f(x_i)&amp; {|y_i-f(x_i)| \leq \delta}\\<br>\delta sign(y_i-f(x_i))&amp; {|y_i-f(x_i)| &gt; \delta}<br>\end{cases}$$
##### d) 分位数损失
它对应的是分位数回归的损失函数，表达式为$$L(y, f(x)) =\sum\limits_{y \geq f(x)}\theta|y - f(x)| + \sum\limits_{y &lt; f(x)}(1-\theta)|y - f(x)|&nbsp;$$
其中$\theta$为分位数，需要我们在回归前指定。对应的负梯度误差为：
$$r(y_i, f(x_i))=<br>\begin{cases}<br>\theta&amp; { y_i \geq f(x_i)}\\<br>\theta - 1 &amp; {y_i &lt; f(x_i) }<br>\end{cases}$$
对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。
三种损失函数对应的梯度表：
![](https://pic2.zhimg.com/80/v2-8cb6283bbf2e1077dfb02daf722c69d9_hd.png)

### 5 GBDT的正则化
&#8195;&#8195;和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。
第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为$\nu$,对于前面的弱学习器的迭代$$f_{k}(x) = f_{k-1}(x) + h_k(x) $$
如果我们加上了正则化项，则有$$f_{k}(x) = f_{k-1}(x) + \nu h_k(x) $$
$\nu$的取值范围为$0 &lt; \nu \leq 1 $。对于同样的训练集学习效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。
&nbsp;
&#8195;&#8195;第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。
&#8195;&#8195;第三种是对于弱学习器即CART回归树进行正则化剪枝。

### 6 优缺点

** 优点**
（1）预测阶段的计算速度快，树与树之间可并行化计算。
（2）在分布稠密的数据集上，泛化能力和表达能力都很好。
（3）采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理如归一化等。
**缺点**
（1）GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
（2）GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
（3）训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。


### 7 GBDT类库 Sklearn参数

#### 7.1 GBDT类库boosting框架参数

&#8195;&#8195;GradientBoostingClassifier和GradientBoostingRegressor的参数绝大部分相同，下面会一起来讲：

&#8195;&#8195;1) **n_estimators**: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。

&#8195;&#8195;2) **learning_rate**: 即每个弱学习器的权重缩减系数v也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为$f_k(x)$=$f_{k−1}(x)$+$νh_k(x)$> $ν$。$v$的取值范围为 0 < $v$ $<=1$。对于同样的训练集拟合效果，较小的意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的开始调参，默认是1。

&#8195;&#8195;3) **subsample**: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。

&#8195;&#8195;4) **init**: 即我们的初始化的时候的弱学习器，拟合对应2.1中的$F_0(x)$，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。

&#8195;&#8195;5) **loss: **即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。
&#8195;&#8195;对于分类模型，有对数似然损失函数"deviance"和指数损失函数"exponential"两者输入选择。默认是对数似然损失函数"deviance"。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的"deviance"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。
&#8195;&#8195;对于回归模型，有均方差"ls", 绝对损失"lad", Huber损失"huber"和分位数损失“quantile”。默认是均方差"ls"。一般来说，如果数据的噪音点不多，用默认的均方差"ls"比较好。如果是噪音点较多，则推荐用抗噪音的损失函数"huber"。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。

&#8195;&#8195;6) **alpha：**这个参数只有GradientBoostingRegressor有，当我们使用Huber损失"huber"和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。

#### 7.2 GBDT类库弱学习器参数

&#8195;&#8195;这里我们再对GBDT的类库弱学习器的重要参数做一个总结。由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。如果你已经很熟悉决策树算法的调参，那么这一节基本可以跳过。不熟悉的朋友可以继续看下去。

&#8195;&#8195;1) 划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑$log_2N$个特征。如果是 "sqrt"或者"auto"意味着划分时最多考虑$\sqrt{N} $个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。

&#8195;&#8195;2) 决策树最大深度**max_depth**: 默认可以不输入，如果不输入的话，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

&#8195;&#8195;3) 内部节点再划分所需最小样本数**min_samples_split**: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

&#8195;&#8195;4) 叶子节点最少样本数**min_samples_leaf**: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

&#8195;&#8195;5）叶子节点最小的样本权重和**min_weight_fraction_leaf**：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

&#8195;&#8195;6) 最大叶子节点数**max_leaf_nodes**: 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

&#8195;&#8195;7) 节点划分最小不纯度**min_impurity_split: ** 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。

### 8 参考资料

[梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)
[GBDT原理与Sklearn源码分析-回归篇](https://blog.csdn.net/qq_22238533/article/details/79185969)
[GBDT原理与Sklearn源码分析-分类篇](https://blog.csdn.net/qq_22238533/article/details/79192579)
[GBDT原理与实践-多分类篇](https://blog.csdn.net/qq_22238533/article/details/79199605)
[当我们在谈论GBDT：Gradient Boosting 用于分类与回归](https://zhuanlan.zhihu.com/p/25257856)
[GBDT的那些事儿](https://zhuanlan.zhihu.com/p/30711812)
[scikit-learn 梯度提升树(GBDT)调参小结](https://www.cnblogs.com/pinard/p/6143927.html)]
《统计学习方法》—李航
《百面机器学习》——葫芦娃
