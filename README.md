# 随机森林详解
**阅读目录**

*   1 什么是随机森林？
*   2 随机森林的生成
*   3 袋外错误率（oob error）
*   4 随机森林的特点
*   5 随机森林工作原理解释的一个简单例子
*   6 参考内容

### 1 什么是随机森林？

&#8195;&#8195;随机森林是一种统计学习理论，它利用Booststrap重抽样方法，从原始样本中抽取多个样本，再对每个bootstrap样本进行决策树建模，然后组合成多课决策树进行预测，并通过投票得出最终的预测结果。
&#8195;&#8195;它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。随机森林有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这是随机森林的主要思想--集成思想的体现。“随机”的含义我们会在下边部分讲到。
&#8195;&#8195;其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。
#### boostrap重抽样
核心思想。在n个原始样本数据的范围内，做有放回的抽样。样本容量保持是n，每个子样本被抽到的概率相等，即为1/n。将样本看作整体，将从样本中抽样得到的子样本称为bootstrap样本。

### 2 随机森林的生成
&#8195;&#8195;随机森林属于集成模型的一种。它由若干棵决策树构成，最终的判断结果由每一棵决策树的结果进行简单投票决定。
步骤：
![image]https://github.com/flysaint/Datawhale-/blob/master/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4.png

![](index_files/2e0705b0-9ffc-470a-92cf-a0cbf3852373.jpg)

#### 数据集的抽取

&#8195;&#8195;在构建随机森林模型的过程中，关键的一步是要从原数据集中多次有放回地抽取一部分样本组成新的训练集，且样本量保持不变。后续每一个决策树模型的构建都是分别基于对应的抽样训练集。

“随机森林”中的“随机”二字主要体现在2方面：
①从整体的训练集中抽取数据时，样本是有放回地抽取。
②组成新的训练集后，再从属性集中无放回地抽取一部分属性进行决策树模型的开发

&#8195;&#8195;这些随机操作能很好地增强模型的泛化能力，有效避免了过拟合的问题。也别其他一些模型所借鉴（例如极大梯度提升树）。

在1中，每次有放回地生成同等样本量的数据集时，大约有1/3的样本没有被选中，留作“袋外数据”。
在2中，假设原有m个属性，在第2步中一般选择$\log2 m个属性进行决策树开发。$

#### 决策树结果的融合

得到若干棵决策树后，会对模型的结果进行融合。在随机森林中，融合的方法通常是简单投票法。假设K棵决策树的投票分别是$t_1,  t_2,…,t_K,  t_i∈{0,1}，最终的分类结果是$
![](index_files/ca3204be-0128-4c99-b444-95b04bd75462.png)

随机森林的输出概率
同时随机森林也支持以概率的形式输出结果：
![](index_files/4a2000cf-3e23-466e-bbc9-25868b3ff4c7.png)

### 3 袋外错误率（oob error）

&#8195;&#8195;上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。

　　随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。

　　我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。

　　而这样的采样特点就允许我们进行oob估计，它的计算方式如下：

　　**（note：以样本为单位）**

　　1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；

　　2）然后以简单多数投票作为该样本的分类结果；

　　3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。


　　oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。

### 4 随机森林的特点
#### 收敛性
![](index_files/15bf6ba7-5267-4a22-81fc-96b30447d9e5.png)

#### 泛化误差上界
![](index_files/992386be-e49e-4034-8705-dfbd45b9a8c6.png)

#### 优点
* 能够有效的运行在大数据集上。
* 能够评估在各个特征在分类问题上的重要性。
* 能够处理缺失值较多的数据。
* 它提供一个实验方法，可以去侦测 variable interactions。
* 对于不平衡的分类数据集来说，它可以平衡误差。

#### 缺点
* 在某些噪音较大的分类或回归问题上会过拟合。
* 对于不同级别属性的数据，级别划分较多的属性会对随机森林产生更大影响，在这类样本上，产生的属性权值是不可靠的。


### 5 随机森林工作原理解释的一个简单例子

**描述**：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。

**　　收入层次 :**

　　　　Band 1 : Below $40,000

　　　　Band 2: $40,000 – 150,000

　　　　Band 3: More than $150,000

　　随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。

**　　CART 1 : Variable Age**

　　![rf1](http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/06/rf1.png)

**　　CART 2 : Variable Gender**

　　![rf2](http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/06/rf2.png)

**　　CART 3 : Variable Education**

　　![rf3](http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/06/rf3.png)

**　　CART 4 : Variable Residence**

　　![rf4](http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/06/rf4.png)

**　　CART 5 : Variable Industry**

　　![rf5](http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/06/rf5.png)

　　我们要预测的某个人的信息如下：

　　1\. Age : 35 years ; 2\. Gender : Male ; 3\. Highest Educational Qualification : Diploma holder; 4\. Industry : Manufacturing; 5\. Residence : Metro.

　　根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况：

　　![DF](http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/06/DF.png)

　　最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（小于$40,000）。
  
### 6 参考内容
# [Machine Learning & Algorithm] 随机森林（Random Forest）](https://www.cnblogs.com/maybe2030/p/4585705.html)

