* 目录
* 1 什么是集成学习？

* 2 集成学习的种类——boosting bagging

* 3 结合策略(平均法，投票法，学习法)

* 4 参考资料

### 1 什么是集成学习？
&#8195;&#8195;将多个分类器的结果统一成一个最终的决策。使用这类策略的机器学习方法统称为集成学习。其中的每个单独的分类器称为基分类器。如同贤明的君主广泛地听取众多谋臣的建议，然后综合考虑，得到最终决策。这里君主的决策就是最终结果，而每个谋臣就是基分类器。
&#8195;&#8195;俗语说“三个臭皮匠，顶一个诸葛亮”，基分类器就类似于“臭皮匠”，即使单一一个“臭皮匠”的决策能力不强，我们有效地把多个“臭皮匠”组织结合起来，其决策能力很有可能超过“诸葛亮”。
![](https://github.com/flysaint/Datawhale-/blob/master/集成学习.png)

### 2 集成学习的种类——boosting bagging

#### Boosting
&#8195;&#8195;Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到
最终结果。

&#8195;&#8195;Boosting的过程很类似于人类——从错误中不断学习。我们学习新知识的过程往往是迭代式的，第一遍学习的时候，我们会记住一部分知识，但往往也会犯一些错误，对于这些错误，我们的印象会很深。第二遍学习的时候，就会针对犯过错误的知识加强学习，以减少类似的错误发生。不断循环往复，直到犯错误的次数减少到很低的程度。

![](https://github.com/flysaint/Datawhale-/blob/master/Boosting.png)

#### bagging

&#8195;&#8195;Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。其中很著名的算法之一是基于决树基分类器的随机森林（Random Forest）。为了让基分类器之间互相独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有交叠）。Bagging方法更像是一个集体决策的过程，每个个体都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠。但由于个体之间存在差异性，最终做出的判断不会完全一致。在最终做决策时，每个个体单独作出判断，再通过投票的方式做出最后的集体决策。

&#8195;&#8195;我们再从消除基分类器的偏差和方差的角度来理解Boosting和Bagging方法的差异。基分类器，有时又被称为弱分类器，因为基分类器的错误率要大于集成分类器。基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛。方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。

&#8195;&#8195;Boosting方法是通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。假设所有基分类器出错的概率是独立的，在某个测试样本上，用简单多数投票方法来集成结果，超过半数基分类器出错的概率会随着基分类器的数量增加而下降。

&#8195;&#8195;如下图所示，Model 1、Model 2、Model 3都是用训练集的一个子集训练出来的，单独来看，它们的决策边界都很曲折，有过拟合的倾向。集成之后的模型（红线所示）的决策边界就比各个独立的模型平滑了，这是由于集成的加权投票方法，减小了方差。
Bagging算法的示意图
![](https://github.com/flysaint/Datawhale-/blob/master/Bagging.png)


### 3 结合策略(平均法，投票法，学习法)
&#8195;&#8195; 有了基分类器后，如何将基分类器的结果组合在一起呢？常见的方法有平均法，投票法，学习法。

#### 平均法

![](https://github.com/flysaint/Datawhale-/blob/master/简单平均法.png)
#### 投票法
**绝对多数投票法(majority voting)**
若某标记得票过半数，则预测为该标记;否则拒绝预测.

**相对多数投票法(plurality voting)**
预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个

**加权投票法(weighted yoting)**
在相对多数投票法的基础上，加入了权值。

#### 学习法stacking
&#8195;&#8195;将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。
&#8195;&#8195;Stacking算法分为2层，第一层是用不同的算法形成T个弱分类器，同时产生一个与原数据集大小相同的新数据集，利用这个新数据集和一个新算法构成第二层的分类器。
![](https://github.com/flysaint/Datawhale-/blob/master/stacking.png)



### 4 参考资料

《机器学习》——周志华

《百面机器学习》——葫芦娃

[集成学习](https://blog.csdn.net/messi_james/article/details/81035991)
